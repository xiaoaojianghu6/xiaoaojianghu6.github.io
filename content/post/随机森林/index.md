---
authors:
- william
date: '2025-04-10'
summary: '决策森林 = 多棵决策树 + 集成策略'
tags: [ML]
title: 随机森林
---

# 随机森林

**决策森林 = 多棵决策树 + 集成策略**

## **Bagging（Bootstrap Aggregating）是什么？**

### **✔ 定义：**

> Bagging = Bootstrap + Aggregating（聚合）
> 
- 使用多个 bootstrap 数据集分别训练多个模型
- 最终对输出做**投票**（分类）或**平均**（回归）

### **🔧 构建流程（简化）：**

1. **从训练集中有放回地抽样**（叫做 Bootstrap 采样）生成 n 个数据子集（每个训练一个树）
    ◦ 剩下没选中的可以作为“验证集” → OOB（out-of-bag）
2. 对每棵树：
    - 随机选择部分特征（不是用所有特征）来划分
    - 用这些样本和特征训练出一棵决策树（一般不剪枝）
3. **分类：** 由所有树投票表决
    - 每棵树都输出一个分类结果
    - 所有树的结果进行**多数投票**
    - 输出“票数最多”的那一类
4. **回归：** 所有树的输出取平均值

主要靠两个“随机性”来提升性能：

- **数据随机性（Bagging）**：每棵树看到的数据不一样，减小了方差（降低过拟合）
- **特征随机性**：每次只挑一部分特征来划分，避免某些强特征主导所有树
    - 如果每棵树都靠它来分裂，其他潜在有用的特征得不到机会
    - 当这个强特征有“噪声”时（测量错误、不稳定），整个森林就跟着翻车
    - 每棵树都会偏向使用同一个“最优特征”最终学出来的树结构差不多
    - 所以“这群树其实意见差不多”，集成投票就失去了意义