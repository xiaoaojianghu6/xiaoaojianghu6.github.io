---
authors:
- william
date: '2025-04-18'
summary: '| **改进** | **LightGBM 的做法** | **有什么用** |'
tags: [LM]
title: LightGBM
---

# LightGBM

## **🔧 LightGBM 相比 XGBoost 有哪些核心改进？**

| **改进** | **LightGBM 的做法** | **有什么用** |
| --- | --- | --- |
| 1️⃣ 样本使用方式 | **基于直方的决策树学习算法（Histogram）** | 把连续特征离散成几个桶，加快计算，降低内存占用 |
| 2️⃣ 样本增长方式 | **Leaf-wise（叶子优先）** | 每次从所有叶子中选出增益最大的叶子进行分裂（而不是按层扩展）→ 更快、更准 |
| 3️⃣ 特征按需 | **GOSS（Gradient-based One-Side Sampling）** | 对梯度小的样本进行采样，只保留梯度大的重要样本 |
| 4️⃣ 特征选择 | **EFB（Exclusive Feature Bundling）** | 把互斥的稀疏特征合并，减少维度 |
| 5️⃣ 支持稀疏数据 | 内置稀疏感知机制（zero-aware） | 自动处理缺失值、稀疏矩阵，无需额外预处理 |

## **📦 LightGBM 和 XGBoost 对比**

| **特性** | **XGBoost** | **LightGBM** |
| --- | --- | --- |
| 树生长策略 | Level-wise（按层） | Leaf-wise（按增益） |
| 特征离散化 | 不做离散化 | 使用 histogram 离散化 |
| 特征选择 | 全部参与 | 支持 GOSS + EFB（更快） |
| 稀疏处理 | 基本支持 | 原生支持（更优） |
| 精度 | 稍弱（在同参数下） | 通常更高（但也更易过拟合） |
| 训练速度 | 较快 | 非常快 |
| 大数据支持 | 好 | **更强！** |
| 稳定性 | 稳定泛化 | 更容易过拟合，需要调参控制 |

## **🧠 实践建议（什么时候用 LightGBM 更香？）**

| **适用场景** | **原因** |
| --- | --- |
| 特征很多、维度稀疏 | EFB + 稀疏支持 |
| 样本量巨大 | GOSS 采样策略节省资源 |
| 需要快速迭代 | 训练快、占内存小 |
| 排序类问题（比如推荐） | 原生支持 Ranking 模式 |
| Kaggle 比赛 | 高分神器🔥（几乎都是 XGB / LGB / CatBoost） |

---

## **🧬 LightGBM 的核心训练机制详解**

### **🔹 1. Leaf-wise 树生长（区别于 XGBoost 的 Level-wise）**

- XGBoost 是“按层扩展”树，每一层都增加一定数量的叶子节点。
- LightGBM 是“从当前最有希望的叶子扩展”，**增益最大的叶子优先分裂**。

> ✅ 优点：更快收敛，准确率更高
> 

> ⚠️ 缺点：容易过拟合（尤其在小数据上）
> 

### **🔹 2. Histogram 算法**

- 把连续特征**离散成 k 个桶**（默认256个）
- 训练过程中不再计算每个值的增益，而是直接在桶上做累积统计

> ✅ 大幅减少内存 & 加快训练
> 

> 类似：从精细画画变成“像素风格涂色”
> 

---

## **✅ LightGBM 的核心流程（以分类为例）**

---

### **🧭 总览流程：**

```
数据输入 → 离散化（Histogram） → 构建第一棵树 → 计算残差（负梯度）
→ 选出最优分裂特征与位置 → 构建下一棵树（继续拟合残差）
→ 多轮迭代 → 最终模型 = 所有树的加权和
```

---

### **数据准备 + 离散化（Histogram 构建）**

- 首先，对所有的连续数值型特征进行分桶（binning)处理（默认256个桶）。
- 例如，把“年龄”这个特征（取值范围 0~100）划分为 256 个桶，那么我们只需要记录：样本属于哪个桶、每个桶的梯度和 Hessian 之和。
- 这一步是 LightGBM 的“提速核心”，把浮点数操作换成整数索引，大幅提高效率。

---

### **迭代训练（Boosting）主流程**

LightGBM 是 Boosting 框架，每轮训练一棵新树，这棵树是为了“修正前一轮的错误”

![](output1.png)

![](output2.png)

![](output3.png)

---

## **📌 总结一页图（知识地图）**

```
🎯 本质：Gradient Boosting 框架
🧠 损失：逐步拟合负梯度（残差）
🌲 树构建：
   - Leaf-wise（按增益优先分裂）
   - 使用 Histogram 离散特征
   - 剪枝 & 正则化抑制过拟合
⚙️ 优化：
   - GOSS（保留高梯度样本）
   - EFB（合并稀疏互斥特征）
📈 模型输出 = 多棵树输出的加权和
```

---

## **🎯 问题 1：分桶（Histogram Binning）到底干了啥？为什么能加速？**

---

### **✅ 背景：我们训练树模型时，需要对每个特征找「最佳切分点」，这很慢！**

比如一个数值特征“年龄”，样本是：

```
[25, 32, 44, 21, 37, 60, 28, 49, 36]
```

如果你要建树，你得枚举每一个可能的分裂点：

```
比如在 25/28、28/32、32/36 之间都试一遍，看谁分裂后信息增益最大
```

> 🚫 这种方式非常慢，尤其是特征多、样本多时，要扫描所有样本。
> 

---

### **✅ 分桶做了什么？**

👉 LightGBM 会在训练前，把“连续值”特征**离散成固定的桶（bin）**。

比如我们设定 4 个桶（实际中常用 255~512 个）：

| **样本值** | **所属桶** |
| --- | --- |
| 21 | bin0 |
| 25 | bin0 |
| 28 | bin1 |
| 32 | bin1 |
| 36 | bin2 |
| 37 | bin2 |
| 44 | bin3 |
| 49 | bin3 |
| 60 | bin3 |

然后它只需要枚举桶之间的边界，比如：

- bin0 vs bin1
- bin1 vs bin2
- bin2 vs bin3

来计算分裂点，而不是枚举所有具体值。

---

## **🍃 问题 2：什么是 Leaf-wise？怎么和 XGBoost 的 Level-wise 不一样？**

---

### **✅ Level-wise：按“层”来生长树（XGBoost）**

假设你现在训练到第一层了（根节点），你会：

```
→ 把所有叶子节点都扩展一层（2个 → 4个 → 8个）
```

这种方式结构整齐，**适合并行计算**，但：

> ❌ 有的分裂没啥用（增益低），也要做
> 

> ❌ 每层都扩，可能做了很多“无效”的工作
> 

---

### **✅ Leaf-wise：LightGBM 的方式：**

### **每次都找出增益最大的叶子扩展**

比如当前的树是这样：

```
      Root
     /    \
    L1    L2
         /  \
       L3   L4
```

它会去看每个叶子的分裂潜力（信息增益大）（分裂谁让混乱减少的多），比如：

| **叶子** | **分裂信息增益** |
| --- | --- |
| L1 | 0.3 |
| L3 | 0.1 |
| L4 | 0.8 ←最大 |

👉 它就**优先分裂 L4**，生成新的更深的叶子。

因为只有继续分裂”错得多”的叶子，模型的整体误差才能下降得最快

---

### **✅ 好处：**

- 精力集中：只扩展对精度贡献最大的地方
- 学得快：收敛速度更快
- 树不规则、可能不平衡，但准确率更高

---

## **🌪️ 问题 3：GOSS 是什么？为什么叫 One-Side Sampling？**

---

### **✅ 背景问题：样本太多，训练太慢 → 要不要采样一部分样本来训练？**

> 普通的随机采样，会把“难样本”和“容易样本”都随机抽一部分出来。
> 

---

### **✅ GOSS 的策略：**

### **不动“难样本”，只从“容易样本”里采样**

- LightGBM 会先算出每个样本的“残差（梯度）”
- 残差大 → 模型错得离谱 → 重要样本
- 残差小 → 模型基本对了 → 次要样本

---

### **🎯 GOSS 策略：**

- 取 **top 20% 残差最大的样本**（保留所有“困难样本”）
- 再从剩下的 80% 里**随机采样一部分“容易样本”**（比如 10%）

这样就能：

> ✅ 保留训练重点样本，提升训练效率
> 

> ✅ 同时保证对全局样本的泛化能力
> 

> ✅ 比随机采样更高效、更有代表性！
> 

---